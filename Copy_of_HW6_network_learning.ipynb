{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashan006/D085/blob/main/Copy_of_HW6_network_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW8il4Qwu5L3"
      },
      "source": [
        "# <h1><center> HW -- AMATH 342 -- Supervised learning</center></h1>\n",
        "\n",
        "Working together is encouraged.  Please do not refer to prior years' solutions.\n",
        "\n",
        "Turn in a write up with your solutions including discussion, plots, and code. Full points require that this is legible, understandably explained, and reasonably organized. \n",
        "\n",
        "If you have difficulty with the assignment, please $\\textbf{come to office hours and get help!}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6bLTBCKu5L_"
      },
      "source": [
        "$\\textbf{Problem 1: Building a multilayer Neural Net}$\n",
        "\n",
        "![image.png](attachment:ee9c858c-6772-49e4-b1f1-af222b04f04b.png)\n",
        "\n",
        "Create a two layer neural network which can solve the XOR problem. To receive full credit, you need do the following\n",
        "* Write down the weights, theta's and the g's for each of your 3 perceptrons. (A total of 9 numbers and 3 functions, although it would be easiest to just reuse the same g)\n",
        "* Just for the four points (0,0), (1,0), (1, 1), (0,1), show the computations that show that your network really outputs [0, 1, 0, 1] for the four data points. That is, write the exact computations to show how the initial x1 and x2's get transformed to a binary label.\n",
        "\n",
        "<br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7Ffrorsbu5MA"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$\\textbf{Problem 2: Training a Perceptron}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "L5eubY7Bu5MB"
      },
      "source": [
        "Train a perceptron using $\\textit{the perceptron training rule}$ to classify the following data. \n",
        "\n",
        "The data measure septal length (column 1) and petal length (column 2) of two types of flowers. These data are stored in the variable '$\\textbf{X}$'. Your model should take $\\textit{septal length}$ and $\\textit{petal length}$ as inputs. Your model should output 1 if the flower is a setosa or -1 if\n",
        "it is a versicolor (two types of Iris flower). These data are in $\\textbf{y}$. Both have been loaded in the cell below for you. There are 50 samples of each flower (so $\\textbf{X}$ is 100 $\\times$ 2). Here are some steps to help guide you.\n",
        "\n",
        "\n",
        "A) Plot the data. Color code the data so that the  setosa flowers are one color and versicolor flowers are a different color. Can a perceptron be used for these data? Why? \n",
        "\n",
        "B) Write a loop which implements the perceptron learning rule. <br>\n",
        "$\\hspace{4mm}$    a) Initialize your weights and bias to random values (say -1 $\\leq$ w $\\leq$ 1). <br>\n",
        "$\\hspace{4mm}$    b) Iterate through all the data points in $\\textbf{X}$ and update the weights and bias for EACH data point. \n",
        "\n",
        "C) What is the number of misclassifications (or error rate) of your perceptron after 10 steps? 30? \n",
        "\n",
        "D) Sometimes, we will train over many \"epochs.\" That is, we loop over all the data several times and just continue using gradient descent. What is the error rate after training over two epochs? \n",
        "\n",
        "E) How does this error rate change as a function of learning rate? Plot the error as a function of the learning rate for 3-5 different learning rates (hint: try logspace to choose learning rates between 0 and 1). Train over at least one epoch on the data. \n",
        "\n",
        "\n",
        "\n",
        "<br><br><br><br>\n",
        "HINTS: <br><br>\n",
        "-Use a learning rate between 0 and 1. <br> <br>\n",
        "-For your perceptron, you can use the following for updating on each step (this is similar to the learning rule we saw in class):\n",
        "\n",
        "$\\Delta w = \\eta(t_{i}-o_{i}) * x_{i}$ for the weights and <br>\n",
        "$\\Delta \\theta = \\eta(t_i-o_i)$ for the bias, where $o_{i}$ is your output, $t_{i}$ is your target output, $\\eta$ is your learning rate, and $x_{i}$ is your input. <br><br><br>\n",
        "-Remember to update both weights: $w_1$ and $w_2$ in your loop.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uECIcGdu5ME"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as import pandas as pd\n",
        "\n",
        "# Load data \n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "y = df.iloc[0:100, 4].values\n",
        "y = np.where(y == 'Iris-setosa', 1, -1)\n",
        "\n",
        "# Sepal length and petal length\n",
        "X = df.iloc[0:100, [0,2]].values\n",
        "\n",
        "# Plot data\n",
        "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')\n",
        "plt.scatter(X[50:, 0], X[50:, 1], color='blue', marker='x', label='versicolor')\n",
        "plt.xlabel('sepal length')\n",
        "plt.ylabel('petal length')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Perceptron learning rule\n",
        "np.random.seed(5)\n",
        "w = np.random.uniform(low=-1, high=1, size=2)\n",
        "b = np.random.uniform(low=-1, high=1)\n",
        "\n",
        "lr = 0.01\n",
        "errors = []\n",
        "\n",
        "for epoch in range(2):\n",
        "    misclassifications = 0\n",
        "    for i in range(X.shape[0]):\n",
        "        x = X[i, :]\n",
        "        target = y[i]\n",
        "\n",
        "        output = np.sign(np.dot(x, w) + b)\n",
        "\n",
        "        if output != target:\n",
        "            misclassifications += 1\n",
        "            update_w = lr * (target - output) * x\n",
        "            update_b = lr * (target - output)\n",
        "            w += update_w\n",
        "            b += update_b\n",
        "\n",
        "    errors.append(misclassifications)\n",
        "\n",
        "print(\"Number of misclassifications after 10 steps: \", errors[9])\n",
        "print(\"Number of misclassifications after 30 steps: \", errors[29])\n",
        "print(\"Number of misclassifications after two epochs: \", errors[-1])\n",
        "\n",
        "# Plot error rate vs learning rate\n",
        "learning_rates = np.logspace(-3, 0, num=5)\n",
        "error_rates = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    w = np.random.uniform(low=-1, high=1, size=2)\n",
        "    b = np.random.uniform(low=-1, high=1)\n",
        "    errors = []\n",
        "\n",
        "    for epoch in range(1):\n",
        "        misclassifications = 0\n",
        "        for i in range(X.shape[0]):\n",
        "            x = X[i, :]\n",
        "            target = y[i]\n",
        "\n",
        "            output = np.sign(np.dot(x, w) + b)\n",
        "\n",
        "            if output != target:\n",
        "                misclassifications += 1\n",
        "                update_w = lr * (target - output) * x\n",
        "                update_b = lr * (target - output)\n",
        "                w += update_w\n",
        "                b += update_b\n",
        "\n",
        "        errors.append(misclassifications)\n",
        "\n",
        "    error_rates.append(errors[0])\n",
        "\n",
        "plt.plot(learning_rates, error_rates, marker='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('Error rate')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#%% Load data \n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "y = df.iloc[0:100, 4].values\n",
        "y = np.where(y == 'Iris-setosa', 1, -1)\n",
        "\n",
        "# sepal length and petal length\n",
        "X = df.iloc[0:100, [0,2]].values\n",
        "\n",
        "\n",
        "\n",
        "#def Perceptron(weights,inputs,bias):   #You can use this to return the output of the Perceptron at each step! Hint: use np.sign (1/-1) instead of 1/0\n",
        "    #return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvaD5jsPu5MH"
      },
      "outputs": [],
      "source": [
        "#Implementing the perceptron learning rule \n",
        "np.random.seed(5)  \n",
        "#wts = \n",
        "#inp= \n",
        "#bias=\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "    inp = X[i,:]\n",
        "    #get model output, and calculate update to weights and biases\n",
        "    #w += update\n",
        "    #bias += update\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data \n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "y = df.iloc[0:100, 4].values\n",
        "y = np.where(y == 'Iris-setosa', 1, -1)\n",
        "\n",
        "# Sepal length and petal length\n",
        "X = df.iloc[0:100, [0,2]].values\n",
        "\n",
        "# Plot data\n",
        "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')\n",
        "plt.scatter(X[50:, 0], X[50:, 1], color='blue', marker='x', label='versicolor')\n",
        "plt.xlabel('sepal length')\n",
        "plt.ylabel('petal length')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Perceptron learning rule\n",
        "np.random.seed(5)\n",
        "w = np.random.uniform(low=-1, high=1, size=2)\n",
        "b = np.random.uniform(low=-1, high=1)\n",
        "\n",
        "lr = 0.01\n",
        "errors = []\n",
        "\n",
        "for epoch in range(2):\n",
        "    misclassifications = 0\n",
        "    for i in range(X.shape[0]):\n",
        "        x = X[i, :]\n",
        "        target = y[i]\n",
        "\n",
        "        output = np.sign(np.dot(x, w) + b)\n",
        "\n",
        "        if output != target:\n",
        "            misclassifications += 1\n",
        "            update_w = lr * (target - output) * x\n",
        "            update_b = lr * (target - output)\n",
        "            w += update_w\n",
        "            b += update_b\n",
        "\n",
        "    errors.append(misclassifications)\n",
        "\n",
        "print(\"Number of misclassifications after 10 steps: \", errors[9])\n",
        "print(\"Number of misclassifications after 30 steps: \", errors[29])\n",
        "print(\"Number of misclassifications after two epochs: \", errors[-1])\n",
        "\n",
        "# Plot error rate vs learning rate\n",
        "learning_rates = np.logspace(-3, 0, num=5)\n",
        "error_rates = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    w = np.random.uniform(low=-1, high=1, size=2)\n",
        "    b = np.random.uniform(low=-1, high=1)\n",
        "    errors = []\n",
        "\n",
        "    for epoch in range(1):\n",
        "        misclassifications = 0\n",
        "        for i in range(X.shape[0]):\n",
        "            x = X[i, :]\n",
        "            target = y[i]\n",
        "\n",
        "            output = np.sign(np.dot(x, w) + b)\n",
        "\n",
        "            if output != target:\n",
        "                misclassifications += 1\n",
        "                update_w = lr * (target - output) * x\n",
        "                update_b = lr * (target - output)\n",
        "                w += update_w\n",
        "                b += update_b\n",
        "\n",
        "        errors.append(misclassifications)\n",
        "\n",
        "    error_rates.append(errors[0])\n",
        "\n",
        "plt.plot(learning_rates, error_rates, marker='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('Error rate')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j27XclJBu5MI"
      },
      "source": [
        "$\\textbf{Problems 3-4, and extra credit: Reinforcement learning}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ym1C4dqFu5MI"
      },
      "source": [
        "3. In the code from the lecture, AMATH342_RL_tutorial_five_arm_bandit_solution.ipynb, change the underlying bonus scores such that a different arm (with index other than 5) returns the maximum bonus. Is the optimal selected action consistent with the arm that produces the highest score? Visualize the initial and optimal policy via the provided code, and turn in your plots as well as a two-sentence explanation of your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "qiuXl32_u5MJ"
      },
      "source": [
        "4.  The value Iteration algorithm is very similar to the policy evaluation step of the policy iteration algorithm, in the sense that one would continue updating the values via the following recursive update rule,\n",
        "![Screen Shot 2023-02-27 at 10.28.40 PM.png](attachment:fec05f87-7689-40f6-9671-711a6aee4498.png)\n",
        "\n",
        "In the value iteration algorithm, we solely iterate over Values associated with the various steps and once converged, we would obtain the optimal policy from the learned optimal values.\n",
        "In the file, AMATH342_RL_tutorial_five_arm_bandit_value_iteration.ipynb, complete the loop in the definition of the value_iteration function by implementing the Value update as expressed above. The game setup is a five-arm bandit just like in class.\n",
        "Which algorithm converges within fewer episodes? Provide your code, and a plot to back up your answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZnVk7hdu5MK"
      },
      "source": [
        "5.  Bonus (optional extra credit): Set up the game object for the board game framework example from the lecture, and implement an iterative algorithm, as for the bandit problem, to solve for an optimal policy.  Provide code as well as plots that illustrate your results."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}